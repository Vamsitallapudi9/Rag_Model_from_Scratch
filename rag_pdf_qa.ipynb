{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence-transformers faiss-cpu transformers langchain beautifulsoup4 requests python-docx PyMuPDF accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs():\n",
    "    # url = \"https://dn790007.ca.archive.org/0/items/atomic-habits-pdfdrive/Atomic%20habits%20%28%20PDFDrive%20%29.pdf\"\n",
    "    url = 'https://obssr.od.nih.gov/sites/obssr/files/Clinical-Trials.pdf'\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch PDF. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    reader = PdfReader(io.BytesIO(response.content))\n",
    "    docs = []\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            docs.append(text.strip())\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264c080cdacf47d980435500ec0272d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 51 pages, Embedding dim: 384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Generate embeddings\n",
    "embed_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "docs = load_docs()\n",
    "if not docs:\n",
    "    raise ValueError(\"No docs loaded! Check PDF loading.\")\n",
    "embeddings = embed_model.encode(docs, convert_to_tensor=False, show_progress_bar=True)\n",
    "embeddings = np.stack(embeddings).astype('float32')\n",
    "print(f'Loaded {len(docs)} pages, Embedding dim: {embeddings.shape[1]}')\n",
    "\n",
    "# 3. Store in FAISS index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "doc_store = {i: doc for i, doc in enumerate(docs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fast Retriever function\n",
    "def retrieve_top_k(query, k=2):\n",
    "    query_emb = embed_model.encode([query])[0].astype('float32')\n",
    "    D, I = index.search(np.array([query_emb]), k)\n",
    "    return [doc_store[i] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load fast, quantized LLM\n",
    "# Use TinyLlama for fast results (much smaller than Mistral 7B)\n",
    "llm_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)  # 4bit quantization for best speed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"Given the following context, answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query):\n",
    "    context_chunks = retrieve_top_k(query, k=2)\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=5000, do_sample=True, top_k=40, temperature=0.7)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract only the FIRST answer after \"Answer:\" and before another \"Question:\"\n",
    "    answer = answer.split(\"Answer:\",1)[-1]\n",
    "    # Stop at first \"Question:\" if it appears\n",
    "    answer = answer.split(\"Question:\")[0]\n",
    "    return answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
      "        <h2>Question:</h2>\n",
      "        <p>Explain phase 3 clinical trials</p>\n",
      "        <h2>Answer:</h2>\n",
      "        <p>Phase 3 clinical trials are the final phase of clinical trials, where a new therapeutic \n",
      "agent is tested in a large patient population against a placebo or standard therapy. This stage \n",
      "assesses the drug's safety, tolerance, and efficacy. The results of this study are used to \n",
      "determine if the drug is safe and effective for use in clinical practice. Phase 3 clinical trials are \n",
      "important because they provide the foundation for the FDA's approval of new drugs.</p>\n",
      "    </div>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"Explain phase 3 clinical trials\"\n",
    "    answer = generate_answer(query)\n",
    "\n",
    "    html_output = f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "        <h2>Question:</h2>\n",
    "        <p>{query}</p>\n",
    "        <h2>Answer:</h2>\n",
    "        <p>{answer}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    print(html_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
